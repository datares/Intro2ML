{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLJT1JzI37EL"
      },
      "source": [
        "# Intro to Deep Learning\n",
        "\n",
        "Now that we have a better hold on traditional ML models from last week, we'll start to look at deep learning.\n",
        "\n",
        "![neural network image](https://miro.medium.com/max/1400/0*_SH7tsNDTkGXWtZb.png\n",
        ")\n",
        "\n",
        "## Why Deep Learning\n",
        "- Deep learning is used commonly today to deal with non-tabular datasets such as images, text, and speech.  \n",
        "    - Common use cases include object recognition (draw bounding boxes around each object in an image), text translation (google translate), and text to speech algorithms like alexa.\n",
        "- We need a different set of algorithms to handle these use cases because \n",
        "usually we need a way to learn features of the input data, and we want to learn features with multiple layers of abstraction\n",
        "    - this contrasts with traditional ML since we usually already have features in our input data (for example tabular data), whereas if we want a model to tell us if an image is a dog or not, then we have no way to easily feed features to the model (eg a dog has 4 legs, a tail, and is fluffy)\n",
        "\n",
        "\n",
        "## Similarities to Traditional ML\n",
        "- We still have the same types of learning: supervised and unsupervised\n",
        "    - reinforcement learning really only works with deep learning\n",
        "\n",
        "\n",
        "## What we're going to work on today\n",
        "- Today we're going to introduce everyone to Keras, a simple deep learning library in python\n",
        "- we'll also start working with non-tabular data domains by talking about how we can build an algorithm to recognize handwritten digits\n",
        "- finally we'll discuss the concept of a Neural Network and a Convolutional Nerual Network\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJRAlqEq7n4z",
        "outputId": "fb6c3fd9-52c1-4b4d-c4a4-2e16aeb7dd91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 16879338153366029070\n",
            "xla_global_id: -1\n",
            ", name: \"/device:GPU:0\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 11320098816\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 2110080074044095204\n",
            "physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7\"\n",
            "xla_global_id: 416903419\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "%matplotlib inline \n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "print(device_lib.list_local_devices()) # this will let us run on the GPU if we have it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "88zd9wni-ldY"
      },
      "outputs": [],
      "source": [
        "# Model / data parameters\n",
        "num_classes = 10\n",
        "input_shape = (28, 28, 1)\n",
        "\n",
        "# the data, split between train and test sets - similar to sklearn\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "# the images are just 2d numpy arrays\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "id": "gXkc0N4s-tCZ",
        "outputId": "e1053c06-7091-4e61-c6e9-6897571d4b08"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "for i in range(2):\n",
        "    img = x_train[i]\n",
        "    cls = y_train[i]\n",
        "    plt.imshow(img, interpolation='nearest', cmap=plt.cm.binary)\n",
        "    plt.xlabel(f\"Data label: {cls}\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHT4LL7UAG-k"
      },
      "source": [
        "# data preprocessing\n",
        "- currently our data is a grayscale image, so each of our values are in the range [0, 255] (RGB colors are 8 bit)\n",
        "    - just as with normal data cleaning, we like to have our values in some predefined range because big numbers can lead to numerical errors when using models, so we'd like to get everything in the range [0, 1] instead"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3C3BWMml-FM4",
        "outputId": "acc710ed-cf67-4be5-95c3-d23bd63f60f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n"
          ]
        }
      ],
      "source": [
        "# Scale images to the [0, 1] range\n",
        "x_train = x_train.astype(\"float32\") / 255\n",
        "x_test = x_test.astype(\"float32\") / 255\n",
        "\n",
        "# Make sure images have shape (28, 28, 1)\n",
        "x_train = np.expand_dims(x_train, -1)\n",
        "x_test = np.expand_dims(x_test, -1)\n",
        "print(\"x_train shape:\", x_train.shape)\n",
        "print(x_train.shape[0], \"train samples\")\n",
        "print(x_test.shape[0], \"test samples\")\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-HjZBwlBDSr"
      },
      "source": [
        "# Building the Model\n",
        "\n",
        "- Now we have to actually build our model.  In deep learning, models are comprised of individual units called layers, which is just a function that takes in inputs and gives an output.  \n",
        "\n",
        "## Convolutions\n",
        "- a convolution is just a spicy matrix multiplication\n",
        "- it has had a lot of success in deep neural networks and computer vision because it mathematically captures the idea of a receptive field in images to recognize features\n",
        "\n",
        "![convolution gif](https://miro.medium.com/max/1400/1*D6iRfzDkz-sEzyjYoVZ73w.gif)\n",
        "\n",
        "- why this works isn't too important for now, but just knowing what it is and what it does is most helpful \n",
        "\n",
        "## Maxpooling \n",
        "- When we've finished applying our convolution we want to only take the most important features from the previous layer.  This is just another simple mathematical operation known as maxpooling\n",
        "\n",
        "![image.gif](https://nico-curti.github.io/NumPyNet/NumPyNet/images/maxpool.gif)\n",
        "\n",
        "\n",
        "## Activation function\n",
        "- Although a layer is just a function, we like to take the output of the layer and pass it through another function called an activation function.\n",
        "- this is usually a nonlinear function that helps scale the output values to some range and help the model generalize to outside data.\n",
        "\n",
        "![image.png](https://miro.medium.com/max/1400/1*XxxiA0jJvPrHEJHD4z893g.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ia1yMJUG-Lzr",
        "outputId": "581184a4-c487-43e9-f572-f291cba69f41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 26, 26, 32)        320       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 13, 13, 32)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 11, 11, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 5, 5, 64)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 1600)              0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 1600)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 10)                16010     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 34,826\n",
            "Trainable params: 34,826\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=input_shape),\n",
        "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(num_classes, activation=\"softmax\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_df3U9uSNzP"
      },
      "source": [
        "# What our model does \n",
        "\n",
        "![model gif](https://cdn-images-1.medium.com/max/600/1*QPRC1lcfYxcWWPAC2hrQgg.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjuQFjo3DMCO"
      },
      "source": [
        "# Running the model\n",
        "- There are a few parameters to discuss before we run the model\n",
        "1. Batch size is how the number of samples processed by the model at each step\n",
        "    - higher batch size means our model sees more data points before we update the parameters, so hopefully we're making less sporadic steps towards the optimum\n",
        "2. Epochs is the total number of times we go over the entire dataset during training.\n",
        "    - in our case epochs is set to 15, so our model sees the entire training set 15 times before we stop training.  Higher values should produce better metrics but it can also lead to overfitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0yFlxjyCyHa",
        "outputId": "fec2f0bc-efb6-4837-b446-f7b195b8b686"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "422/422 [==============================] - 6s 10ms/step - loss: 0.3670 - accuracy: 0.8862 - val_loss: 0.0827 - val_accuracy: 0.9772\n",
            "Epoch 2/15\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.1142 - accuracy: 0.9651 - val_loss: 0.0565 - val_accuracy: 0.9842\n",
            "Epoch 3/15\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.0878 - accuracy: 0.9727 - val_loss: 0.0478 - val_accuracy: 0.9873\n",
            "Epoch 4/15\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.0747 - accuracy: 0.9764 - val_loss: 0.0437 - val_accuracy: 0.9880\n",
            "Epoch 5/15\n",
            "422/422 [==============================] - 4s 8ms/step - loss: 0.0652 - accuracy: 0.9802 - val_loss: 0.0406 - val_accuracy: 0.9883\n",
            "Epoch 6/15\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.0592 - accuracy: 0.9814 - val_loss: 0.0378 - val_accuracy: 0.9893\n",
            "Epoch 7/15\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.0540 - accuracy: 0.9833 - val_loss: 0.0341 - val_accuracy: 0.9897\n",
            "Epoch 8/15\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.0497 - accuracy: 0.9846 - val_loss: 0.0358 - val_accuracy: 0.9905\n",
            "Epoch 9/15\n",
            "422/422 [==============================] - 4s 8ms/step - loss: 0.0469 - accuracy: 0.9854 - val_loss: 0.0332 - val_accuracy: 0.9898\n",
            "Epoch 10/15\n",
            "422/422 [==============================] - 4s 8ms/step - loss: 0.0441 - accuracy: 0.9861 - val_loss: 0.0309 - val_accuracy: 0.9902\n",
            "Epoch 11/15\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.0427 - accuracy: 0.9860 - val_loss: 0.0296 - val_accuracy: 0.9915\n",
            "Epoch 12/15\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.0380 - accuracy: 0.9871 - val_loss: 0.0296 - val_accuracy: 0.9920\n",
            "Epoch 13/15\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.0389 - accuracy: 0.9877 - val_loss: 0.0293 - val_accuracy: 0.9923\n",
            "Epoch 14/15\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.0365 - accuracy: 0.9886 - val_loss: 0.0287 - val_accuracy: 0.9928\n",
            "Epoch 15/15\n",
            "422/422 [==============================] - 4s 9ms/step - loss: 0.0343 - accuracy: 0.9891 - val_loss: 0.0291 - val_accuracy: 0.9907\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb4227de390>"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batch_size = 128\n",
        "epochs = 15\n",
        "\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0bws8LARrnu",
        "outputId": "5133f123-21a3-47c8-e2bb-8627089efffd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss: 0.026940423995256424\n",
            "Test accuracy: 0.9907000064849854\n"
          ]
        }
      ],
      "source": [
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Test loss:\", score[0])\n",
        "print(\"Test accuracy:\", score[1])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-AqRrq7QNIiC"
      },
      "source": [
        "# How learning happens\n",
        "- so far we've been able to actually train a deep learning model to recognize digits to almost perfect accuracy, but the question of how the \"learning\" process actually happens still eludes us\n",
        "\n",
        "## it's actually not that hard\n",
        "- We can describe the learning process in 3 steps\n",
        "1. We calculate how bad our current model is using the loss function\n",
        "    - the loss function $L$ is a scalar function that takes as inputs our model's output $\\hat{y}$ and the true value $y$ and returns a scalar value\n",
        "    - it's just a function that measures the distance between our output and the true value\n",
        "\n",
        "$L: \\mathbb{R}^n * \\mathbb{R}^n \\rightarrow \\mathbb{R}$\n",
        "\n",
        "2. We calculate the derivative of our loss with respect to every parameter in our model\n",
        "    - if you know about simple algorithms like Newton's Method and Fixed Point Iteration, this is essentially what we're doing\n",
        "\n",
        "3. We subtract the derivative times a small constant $\\alpha$ from the current parameter's value\n",
        "\n",
        "$ x_{n+1} = x_{n} - \\alpha * \\frac{\\partial L}{\\partial x_n}$\n",
        "when $x_{n}$ converges to during training $x^*$, we're at a local optimum, which can be shown by noting that\n",
        "\n",
        "$$\\begin{aligned}\n",
        " x^* &= x^* - \\alpha * \\frac{\\partial L}{\\partial x_n} \\\\ \n",
        "    \\alpha \\frac{\\partial L}{\\partial x_n} &= 0 \\\\ \n",
        "    \\frac{\\partial L}{\\partial x_n} &= 0\n",
        "\\end{aligned}$$\n",
        "\n",
        "and we know that when the derivative is zero, we're at an optimal value.  Whether or not this optimum is global is a much harder question, and it's possible that during training your model converges to a non-global optimum or a saddle point.\n",
        "\n",
        "In reality, this is all handled by the library, and we never have to think about the backpropogation or optimization steps.  Plus, these calculations get a ton harder since most neural networks are actually a ton of matrix multiplications, so we'd be taking the derivative of a scalar value with respect to a bunch of matricies (good luck with that)\n",
        "\n",
        "This process is called Stochastic Gradient Descent and it's best illustrated with a picture \n",
        "![gradient descent gif](https://hackernoon.com/hn-images/0*D7zG46WrdKx54pbU.gif)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Part2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
